{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_manager import section1, section2, single_model\n",
    "import optimizer as opt\n",
    "import dataset_creator\n",
    "from constants import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset1_generator = dataset_creator.Dataset1()\n",
    "dataset1_generator.generate_dataset(train_size=30000, test_size=10000, whole_dataset=True)\n",
    "\n",
    "import gc\n",
    "del dataset1_generator\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = opt.BayesianOptimizer()\n",
    "\n",
    "\n",
    "params = {\n",
    "    'min_conv_layers': 1,\n",
    "    'max_conv_layers': 5,\n",
    "    'min_dense_layers': 1,\n",
    "    'max_dense_layers': 5,\n",
    "    'min_conv_size': 4,\n",
    "    'max_conv_size': 256,\n",
    "    'min_dense_size': 4,\n",
    "    'max_dense_size': 256,\n",
    "    'epochs': 50,\n",
    "    'flatten_type': ['global_average'],\n",
    "    'activation': ['relu'],\n",
    "    'optimizer': ['adam'],\n",
    "    'learning_rate': [0.001],\n",
    "    'dropout': [0.01],\n",
    "}\n",
    "\n",
    "\n",
    "model = single_model()\n",
    "\n",
    "best_trial = optim.optimize(model=model, max_trials=500, target=0.9, initial_params=params, write_to_file=True, penalty_blob='divide')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = opt.BayesianOptimizer(starting_values)\n",
    "\n",
    "\n",
    "params = {\n",
    "    'min_conv_layers': 1,\n",
    "    'max_conv_layers': 5,\n",
    "    'min_dense_layers': 1,\n",
    "    'max_dense_layers': 5,\n",
    "    'min_conv_size': 4,\n",
    "    'max_conv_size': 256,\n",
    "    'min_dense_size': 4,\n",
    "    'max_dense_size': 256,\n",
    "    'epochs': 50,\n",
    "    'flatten_type': ['global_average'],\n",
    "    'activation': ['relu'],\n",
    "    'optimizer': ['adam'],\n",
    "    'learning_rate': [0.001],\n",
    "    'dropout': [0.01],\n",
    "}\n",
    "\n",
    "\n",
    "model = section1()\n",
    "\n",
    "best_trial = optim.optimize(model=model, max_trials=500, target=5, initial_params=params, write_to_file=True, penalty_blob='multiply')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def load_params(file_path):\n",
    "    \"\"\"Load the parameters from a JSON file.\"\"\"\n",
    "    with open(file_path, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "def transform_params(params):\n",
    "    \"\"\"Transform the parameters to match the desired structure.\"\"\"\n",
    "    best_conv = [param for key, param in params.items() if key.startswith('conv')]\n",
    "    best_dense = [param for key, param in params.items() if key.startswith('dense')]\n",
    "\n",
    "    # Pass the rest of the parameters unchanged, and only modify conv and dense layers\n",
    "    params['conv_layers'] = best_conv\n",
    "    params['dense_layers'] = best_dense\n",
    "    return params\n",
    "\n",
    "def params_generator(data):\n",
    "    \"\"\"Yield the next set of parameters from the data.\"\"\"\n",
    "    for entry in data:\n",
    "        yield (transform_params(entry[\"params\"]), entry[\"trial_number\"])\n",
    "\n",
    "def update_mae(params, n):\n",
    "    model = section1()\n",
    "    model.initialise_data_and_model(train_params=params, weights=SECT1_CHECKPOINT_FOLDER + f'regression_model_optimize_{n}.weights.h5')\n",
    "    model.model.load_weights(SINGLE_CHECKPOINT_FOLDER + f'single_model_optimize_{n}.weights.h5')\n",
    "    train_params = {\n",
    "        \"epochs\": 1\n",
    "    }\n",
    "    _,_,_,history =model.train(params=train_params)\n",
    "\n",
    "    mae = history['val_mean_absolute_error'][0]\n",
    "    crop_amount = 32\n",
    "    grace = int(round(mae/2, 0))\n",
    "    \n",
    "    change_cropped_image_shape([crop_amount+2*grace,crop_amount+2*grace,1])\n",
    "    CROPPED_IMAGE = {\n",
    "        'image_shape': [crop_amount+2*grace,crop_amount+2*grace,1],\n",
    "        'input_shape': tuple([crop_amount+2*grace,crop_amount+2*grace,1])\n",
    "    }\n",
    "    print(CROPPED_IMAGE)\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Assuming the JSON data is saved in 'bayesian_results.json'\n",
    "    file_path = \"H:\\\\aless\\\\Documents\\\\Python_Scripts\\\\Matur\\\\matura-private-main\\\\matura-private-1\\\\results1\\\\sect1.json\"\n",
    "    #file_path = \"H:\\\\aless\\\\Documents\\\\Python_Scripts\\\\Matur\\\\matura-private-main\\\\matura-private-1\\\\resilt2\\\\sect1.json\"\n",
    "\n",
    "    # Load the data\n",
    "    data = load_params(file_path)\n",
    "\n",
    "    # Create the generator\n",
    "    generator = params_generator(data)\n",
    "\n",
    "    # Iterate through the generator\n",
    "    best_time = float(\"inf\")\n",
    "    best_iter = 0\n",
    "    best_params = {}\n",
    "    for i, (params, n) in enumerate(generator):\n",
    "        print(\"Next Parameters:\", params, \"trial number\", n)\n",
    "\n",
    "        #update_mae(params, n)\n",
    "\n",
    "        #dataset2_gen = dataset_creator.Dataset2(weights=\"H:\\\\aless\\\\Documents\\\\Python_Scripts\\\\Matur\\\\matura-private-main\\\\matura-private-1\\\\resilt2\\\\best_sect1.weights.h5\", trainable_params = params)\n",
    "        dataset2_gen = dataset_creator.Dataset2(weights=\"H:\\\\aless\\\\Documents\\\\Python_Scripts\\\\Matur\\\\matura-private-main\\\\matura-private-1\\\\results1\\\\best_sect1.weights.h5\", trainable_params = params)\n",
    "        dataset2_gen.generate_dataset()\n",
    "\n",
    "\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = opt.BayesianOptimizer()\n",
    "\n",
    "\n",
    "params = {\n",
    "    'min_conv_layers': 1,\n",
    "    'max_conv_layers': 3,\n",
    "    'min_dense_layers': 1,\n",
    "    'max_dense_layers': 3,\n",
    "    'min_conv_size': 4,\n",
    "    'max_conv_size': 128,\n",
    "    'min_dense_size': 4,\n",
    "    'max_dense_size': 128,\n",
    "    'epochs': 50,\n",
    "    'flatten_type': ['global_average'],\n",
    "    'activation': ['relu'],\n",
    "    'optimizer': ['adam'],\n",
    "    'learning_rate': [0.001],\n",
    "    'dropout': [0.01],\n",
    "}\n",
    "\n",
    "\n",
    "model = section2()\n",
    "\n",
    "best_trial = optim.optimize(model=model, max_trials=500, target=0.9, initial_params=params, write_to_file=True, penalty_blob='divide')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
